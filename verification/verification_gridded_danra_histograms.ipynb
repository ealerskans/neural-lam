{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5d8017",
   "metadata": {},
   "source": [
    "##  Gridded Model Verification\n",
    "\n",
    "This script verifies output from a ML-based foundation model versus a\n",
    "traditional NWP system for the atmospheric system. The defaults set at the top of\n",
    "this script are tailored to the Alps-Clariden HPC system at CSCS.\n",
    "- The NWP-model is called DANRA-forecasts and is initialised with the analysis. Only surface level data is available, and up to 18 h lead time.\n",
    "- The ML-model is called Neural-LAM and is initialised from the DANRA reanalysis.\n",
    "- The Ground Truth is the same deterministic DANRA reanalysis as was used to train the ML-model.\n",
    "- The boundary data for both models is IFS HRES from ECMWF, where the NWP-model got 6 hourly boundary updates (?) and the ML model 12 hourly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0311b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import dask\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.distributed import LocalCluster, Client\n",
    "import numpy as np\n",
    "from dask.diagnostics import ProgressBar\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from pysteps.verification.salscores import sal  # requires scikit-image\n",
    "from scipy.stats import kurtosis, skew, wasserstein_distance\n",
    "from scores.categorical import ThresholdEventOperator as TEO\n",
    "from scores.continuous import (\n",
    "    mae,\n",
    "    mean_error,\n",
    "    mse,\n",
    "    rmse,\n",
    ")\n",
    "from scores.continuous.correlation import pearsonr\n",
    "from scores.spatial import fss_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a806b47",
   "metadata": {},
   "source": [
    "**--------> Enter all your user settings in the cell below. <--------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf621a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFAULTS ###\n",
    "# This config will be applied to the data before any plotting. The data will be\n",
    "# sliced and indexed according to the values in this config.The whole analysis and\n",
    "# plotting will be done on the reduced data.\n",
    "\n",
    "# IF YOUR DATA HAS DIFFERENT DIMENSIONS OR NAMES, PLEASE ADJUST THE CELLS BELOW\n",
    "# MAKE SURE THE XARRAY DATASETS LOOK OKAY BEFORE RUNNING CHAPTER 1-4\n",
    "\n",
    "# This path should point to the data that was used to train the model (default is mdp-datastore)\n",
    "PATH_GROUND_TRUTH = \"danra_test_gt.zarr\"\n",
    "# This path should point to the NWP forecast data in zarr format\n",
    "PATH_NWP = \"danra_test_nwp_forecasts.zarr\"\n",
    "# This path should point to the ML forecast data in zarr format (e.g. produced by neural-lam in `eval` mode)\n",
    "PATH_ML = \"48h_eval_test_7deg_rect_hi4_2867.zarr\"\n",
    "# This path should point to the boundary data in zarr format (default is MDP-datastore)\n",
    "PATH_BOUNDARY = \"ifs_test_boundary.zarr\"\n",
    "\n",
    "# elapsed forecast duration in steps for the forecast - [0] refers to the first forecast step at t+1\n",
    "# this should be a list of integers\n",
    "ELAPSED_FORECAST_DURATION = list(range(16))\n",
    "ELAPSED_FORECAST_DURATION_SHORT = list(range(6)) # 18 h, matching NWP forecasts\n",
    "ELAPSED_FORECAST_DURATION_PLOT = [1,3,5] # ELS to plot\n",
    "ELAPSED_FORECAST_DURATION_VERTICAL = [0, 3, 7, 11, 15] # For vertical profiles\n",
    "\n",
    "# Select specific start_times for the forecast. This is the start and end of\n",
    "# a slice in xarray. The start_time is included, the end_time is excluded.\n",
    "# This should be a list of two strings in the format \"YYYY-MM-DDTHH:MM:SS\"\n",
    "# Should be handy to evaluate certain dates, e.g. for a case study of a storm\n",
    "#START_TIMES = [\"2020-02-07T00:00:00\", \"2020-02-10T00:00:00\"] # 7 init times, ~1% of full year\n",
    "START_TIMES = [\"2019-10-30T00:00\", \"2020-10-23T12:00\"] # Full year, 720 init times\n",
    "\n",
    "# Select specific plot times for the forecast (will be used to create maps for all variables)\n",
    "# This only affect chapter one with the plotting of the maps\n",
    "# Map creation takes a lot of time so this is limited to a single time step\n",
    "# Simply rerun these cells and chapter one for more time steps\n",
    "PLOT_TIME = \"2020-02-09T12:00:00\"\n",
    "\n",
    "# Selection spatial grid in projection\n",
    "# This is used to slice the data to a specific region\n",
    "# This is in projection of the ground truth data\n",
    "# The default is the whole domain [None, None]\n",
    "X = [None, None]\n",
    "Y = [None, None]\n",
    "\n",
    "# Map projection settings for plotting\n",
    "# This is the projection of the ground truth data\n",
    "PROJECTION = ccrs.LambertConformal(\n",
    "    central_longitude=25.0,\n",
    "    central_latitude=56.7,\n",
    "    standard_parallels=[56.7, 56.7],\n",
    "    globe=ccrs.Globe(\n",
    "        semimajor_axis=6367470.0,\n",
    "        semiminor_axis=6367470.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Define how variables map between different data sources\n",
    "\n",
    "# Define here which of the variables are available in the ground truth data\n",
    "# The keys are the names of the variables in the ground truth data\n",
    "# The values are the conventional names, used in this notebook\n",
    "VARIABLES_GROUND_TRUTH = {\n",
    "    # Surface and near-surface variables\n",
    "    \"t2m\": \"temperature_2m\",\n",
    "    \"u10m\": \"wind_u_10m\",\n",
    "    \"v10m\": \"wind_v_10m\",\n",
    "    \"pres_seasurface\": \"pressure_sea_level\",\n",
    "    \"pres0m\": \"surface_pressure\",\n",
    "    \"swavr0m\": \"surface_net_shortwave_radiation\",\n",
    "    \"lwavr0m\": \"surface_net_longwave_radiation\",\n",
    "    # Upper air variables - U component\n",
    "    # \"u100\": \"wind_u_100hPa\",\n",
    "    \"u200\": \"wind_u_200hPa\",\n",
    "    # \"u400\": \"wind_u_400hPa\",\n",
    "    # \"u600\": \"wind_u_600hPa\",\n",
    "    \"u700\": \"wind_u_700hPa\",\n",
    "    # \"u850\": \"wind_u_850hPa\",\n",
    "    # \"u925\": \"wind_u_925hPa\",\n",
    "    # \"u1000\": \"wind_u_1000hPa\",\n",
    "    # Upper air variables - V component\n",
    "    # \"v100\": \"wind_v_100hPa\",\n",
    "    \"v200\": \"wind_v_200hPa\",\n",
    "    # \"v400\": \"wind_v_400hPa\",\n",
    "    # \"v600\": \"wind_v_600hPa\",\n",
    "    \"v700\": \"wind_v_700hPa\",\n",
    "    # \"v850\": \"wind_v_850hPa\",\n",
    "    # \"v925\": \"wind_v_925hPa\",\n",
    "    # \"v1000\": \"wind_v_1000hPa\",\n",
    "    # Upper air variables - Pressure\n",
    "    # \"z100\": \"geopotential_100hPa\",\n",
    "    # \"z200\": \"geopotential_200hPa\",\n",
    "    # \"z400\": \"geopotential_400hPa\",\n",
    "    # \"z600\": \"geopotential_600hPa\",\n",
    "    \"z700\": \"geopotential_700hPa\",\n",
    "    # \"z850\": \"geopotential_850hPa\",\n",
    "    # \"z925\": \"geopotential_925hPa\",\n",
    "    # \"z1000\": \"geopotential_1000hPa\",\n",
    "    # Upper air variables - Temperature\n",
    "    # \"t100\": \"temperature_100hPa\",\n",
    "    # \"t200\": \"temperature_200hPa\",\n",
    "    # \"t400\": \"temperature_400hPa\",\n",
    "    # \"t600\": \"temperature_600hPa\",\n",
    "    \"t700\": \"temperature_700hPa\",\n",
    "    # \"t850\": \"temperature_850hPa\",\n",
    "    # \"t925\": \"temperature_925hPa\",\n",
    "    # \"t1000\": \"temperature_1000hPa\",\n",
    "    # Upper air variables - Relative Humidity\n",
    "    # \"r100\": \"relative_humidity_100hPa\",\n",
    "    \"r200\": \"relative_humidity_200hPa\",\n",
    "    # \"r400\": \"relative_humidity_400hPa\",\n",
    "    # \"r600\": \"relative_humidity_600hPa\",\n",
    "    \"r700\": \"relative_humidity_700hPa\",\n",
    "    # \"r850\": \"relative_humidity_850hPa\",\n",
    "    # \"r925\": \"relative_humidity_925hPa\",\n",
    "    # \"r1000\": \"relative_humidity_1000hPa\",\n",
    "    # Upper air variables - Vertical velocity\n",
    "    # \"tw100\": \"vertical_velocity_100hPa\",\n",
    "    \"tw200\": \"vertical_velocity_200hPa\",\n",
    "    # \"tw400\": \"vertical_velocity_400hPa\",\n",
    "    # \"tw600\": \"vertical_velocity_600hPa\",\n",
    "    \"tw700\": \"vertical_velocity_700hPa\",\n",
    "    # \"tw850\": \"vertical_velocity_850hPa\",\n",
    "    # \"tw925\": \"vertical_velocity_925hPa\",\n",
    "    # \"tw1000\": \"vertical_velocity_1000hPa\",\n",
    "}\n",
    "\n",
    "REQUIRED_LEVELS = [\n",
    "    100,\n",
    "    200,\n",
    "    400,\n",
    "    600,\n",
    "    700,\n",
    "    850,\n",
    "    925,\n",
    "    1000,\n",
    "]\n",
    "\n",
    "# Since the default ground_truth is the datastore that was used for model training\n",
    "# the variables are identical to the VARIABLES_GROUND_TRUTH\n",
    "VARIABLES_ML = VARIABLES_GROUND_TRUTH\n",
    "\n",
    "# For the NWP-Forecast only a limited set of variables is available\n",
    "# These variables are mapped to the same conventional names\n",
    "# The script is flexible and will only calculate the NWP-metrics for the variables that are available\n",
    "# The script will not break if some of the variables are not available\n",
    "VARIABLES_NWP = {\n",
    "    \"t2m\": \"temperature_2m\",\n",
    "    \"u10m\": \"wind_u_10m\",\n",
    "    \"v10m\": \"wind_v_10m\",\n",
    "    \"pres_seasurface\": \"seasurface_pressure\",\n",
    "}\n",
    "\n",
    "# These variables are only used for chapter 1, the mapplots.\n",
    "# They will be plotted for the ground truth, NWP and ML\n",
    "VARIABLES_BOUNDARY = {\n",
    "    # Surface and near-surface variables\n",
    "    \"mean_sea_level_pressure\": \"pressure_sea_level\",\n",
    "    \"2m_temperature\": \"temperature_2m\",\n",
    "    \"10m_u_component_of_wind\": \"wind_u_10m\",\n",
    "    \"10m_v_component_of_wind\": \"wind_v_10m\",\n",
    "    \"surface_pressure\": \"surface_pressure\",\n",
    "    # Upper air variables - U component\n",
    "    \"u_component_of_wind100\": \"wind_u_100hPa\",\n",
    "    \"u_component_of_wind200\": \"wind_u_200hPa\",\n",
    "    \"u_component_of_wind400\": \"wind_u_400hPa\",\n",
    "    \"u_component_of_wind600\": \"wind_u_600hPa\",\n",
    "    \"u_component_of_wind700\": \"wind_u_700hPa\",\n",
    "    \"u_component_of_wind850\": \"wind_u_850hPa\",\n",
    "    \"u_component_of_wind925\": \"wind_u_925hPa\",\n",
    "    \"u_component_of_wind1000\": \"wind_u_1000hPa\",\n",
    "    # Upper air variables - V component\n",
    "    \"v_component_of_wind100\": \"wind_v_100hPa\",\n",
    "    \"v_component_of_wind200\": \"wind_v_200hPa\",\n",
    "    \"v_component_of_wind400\": \"wind_v_400hPa\",\n",
    "    \"v_component_of_wind600\": \"wind_v_600hPa\",\n",
    "    \"v_component_of_wind700\": \"wind_v_700hPa\",\n",
    "    \"v_component_of_wind850\": \"wind_v_850hPa\",\n",
    "    \"v_component_of_wind925\": \"wind_v_925hPa\",\n",
    "    \"v_component_of_wind1000\": \"wind_v_1000hPa\",\n",
    "    # Upper air variables - Temperature\n",
    "    \"temperature100\": \"temperature_100hPa\",\n",
    "    \"temperature200\": \"temperature_200hPa\",\n",
    "    \"temperature400\": \"temperature_400hPa\",\n",
    "    \"temperature600\": \"temperature_600hPa\",\n",
    "    \"temperature700\": \"temperature_700hPa\",\n",
    "    \"temperature850\": \"temperature_850hPa\",\n",
    "    \"temperature925\": \"temperature_925hPa\",\n",
    "    \"temperature1000\": \"temperature_1000hPa\",\n",
    "    # Upper air variables - Vertical velocity\n",
    "    \"vertical_velocity100\": \"vertical_velocity_100hPa\",\n",
    "    \"vertical_velocity200\": \"vertical_velocity_200hPa\",\n",
    "    \"vertical_velocity400\": \"vertical_velocity_400hPa\",\n",
    "    \"vertical_velocity600\": \"vertical_velocity_600hPa\",\n",
    "    \"vertical_velocity700\": \"vertical_velocity_700hPa\",\n",
    "    \"vertical_velocity850\": \"vertical_velocity_850hPa\",\n",
    "    \"vertical_velocity925\": \"vertical_velocity_925hPa\",\n",
    "    \"vertical_velocity1000\": \"vertical_velocity_1000hPa\",\n",
    "    # Upper air variables - Geopotential\n",
    "    \"geopotential100\": \"geopotential_100hPa\",\n",
    "    \"geopotential200\": \"geopotential_200hPa\",\n",
    "    \"geopotential400\": \"geopotential_400hPa\",\n",
    "    \"geopotential600\": \"geopotential_600hPa\",\n",
    "    \"geopotential700\": \"geopotential_700hPa\",\n",
    "    \"geopotential850\": \"geopotential_850hPa\",\n",
    "    \"geopotential925\": \"geopotential_925hPa\",\n",
    "    \"geopotential1000\": \"geopotential_1000hPa\",\n",
    "}\n",
    "\n",
    "\n",
    "# These variables will be used as `basename` for the vertical profiles.\n",
    "# Since the input of the zarr archives is expected to have data vars that are 2D in space\n",
    "# we need some base_name prefix to create the 3D variables\n",
    "VARIABLES_3D = [\n",
    "    \"wind_u\",\n",
    "    \"wind_v\",\n",
    "    \"geopotential\",\n",
    "    \"temperature\",\n",
    "    \"relative_humidity\",\n",
    "    \"vertical_velocity\",\n",
    "]\n",
    "\n",
    "# Add units dictionary after the imports\n",
    "# units from zarr archives are not reliable and should rather be defined here\n",
    "VARIABLE_UNITS = {\n",
    "    # Surface and near-surface variables\n",
    "    \"temperature_2m\": \"K\",\n",
    "    \"wind_u_10m\": \"m/s\",\n",
    "    \"wind_v_10m\": \"m/s\",\n",
    "    \"pressure_sea_level\": \"Pa\",\n",
    "    \"surface_pressure\": \"Pa\",\n",
    "    \"precipitation\": \"mm/h\",\n",
    "    #\"surface_sensible_heat_flux\": \"W/m²\",\n",
    "    \"surface_net_shortwave_radiation\": \"W/m²\",\n",
    "    \"surface_net_longwave_radiation\": \"W/m²\",\n",
    "    # Upper air variables\n",
    "    \"wind_u_level\": \"m/s\",\n",
    "    \"wind_v_level\": \"m/s\",\n",
    "    \"geopotential_level\": \"m²/s²\",\n",
    "    \"temperature_level\": \"K\",\n",
    "    \"relative_humidity_level\": \"%\",\n",
    "    \"vertical_velocity_level\": \"Pa/s\",\n",
    "}\n",
    "\n",
    "# Define Thresholds for the ETS metric (Equitable Threat Score)\n",
    "# These are calculated for wind and precipitation if available\n",
    "# The score creates contingency tables for different thresholds\n",
    "# The ETS is calculated for each threshold and the results are plotted\n",
    "# The default thresholds are [0.1, 1, 5] for precipitation and [2.5, 5, 10] for wind\n",
    "THRESHOLDS_PRECIPITATION = [0.1, 1, 5]  # mm/h\n",
    "THRESHOLDS_WIND = [2.5, 5, 10]  # m/s\n",
    "\n",
    "# Define the metrics to compute for the verification\n",
    "# Some additional verifications will always be computed if the repsective vars\n",
    "# are available in the data\n",
    "METRICS = [\n",
    "    # \"MAE\",\n",
    "    \"RMSE\",\n",
    "    # \"MSE\",\n",
    "    \"ME\",\n",
    "    \"STDEV_ERR\",\n",
    "    \"RelativeMAE\",\n",
    "    # \"RelativeRMSE\",\n",
    "    # \"PearsonR\",\n",
    "    # \"Wasserstein\",\n",
    "]\n",
    "\n",
    "# This setting is relevant for the mapplots in chapter 1\n",
    "# Higher levels of ZOOM will zoom in on the map, cropping the boundary\n",
    "BORDER_WIDTH = 300000 # in m\n",
    "ZOOM = 1  # Unused\n",
    "\n",
    "# For some chapters a random seed is required to reproduce the results\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# The DPI used in all plots in the notebook, export to pdf will always be 300 DPI\n",
    "DPI = 100\n",
    "\n",
    "# Subsample the data for faster plotting, 0.1 refers to 10% of the ml/nwp data\n",
    "# sampled along the start_time, x and y dimensions. If you calculate the FSS\n",
    "# metrics you would want to limit subsampling to the time-dimensions! There is a\n",
    "# trade-off between speed and accuracy, that each user has to find.\n",
    "SUBSAMPLE_FRACTION = 1.0\n",
    "\n",
    "# If the data should be loaded into memory. Makes following calculations faster\n",
    "# but requires enough memory to hold the data.\n",
    "PRECOMPUTE_DATA = False\n",
    "\n",
    "# Subsample the data for FSS threshold calculation, 1e7 refers to the number of elements\n",
    "# This is not critical, as it is only used to calculate the 90% threshold\n",
    "# for the FSS based on the ground truth data\n",
    "SUBSAMPLE_FSS_THRESHOLD = 1e7\n",
    "\n",
    "# Takes a long time, but if you see NaN in your output, you can set this to True\n",
    "# This will check if there are any missing values in the data further below\n",
    "# THIS NOTEBOOK WILL ONLY WORK RELIABLY IF THERE ARE NO MISSING VALUES\n",
    "# If there are missing values, you have to interpolate them or remove them\n",
    "CHECK_MISSING = False\n",
    "\n",
    "# Font sizes for consistent plotting (different fig-sizes wil require different font sizes)\n",
    "FONT_SIZES = {\n",
    "    \"axes\": 24,  # Axis labels and titles\n",
    "    \"ticks\": 24,  # Tick labels\n",
    "    \"legend\": 22,  # Legend text\n",
    "    \"cbar\": 24,  # Colorbar labels\n",
    "    \"suptitle\": 24,  # Figure suptitle\n",
    "    \"title\": 24,  # Axes titles\n",
    "    \"stats\": 22,  # Statistics text in plots\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for plots and tables\n",
    "Path(\"plots\").mkdir(exist_ok=True)\n",
    "Path(\"tables\").mkdir(exist_ok=True)\n",
    "\n",
    "# Colorblind-friendly color palette\n",
    "COLORS = {\n",
    "    \"gt\": \"#000000\",  # Black\n",
    "    \"ml\": \"#E69F00\",  # Orange\n",
    "    \"nwp\": \"#56B4E9\",  # Light blue\n",
    "    \"error\": \"#CC79A7\",  # Pink\n",
    "}\n",
    "\n",
    "# Line styles and markers for accessibility\n",
    "LINE_STYLES = {\n",
    "    \"gt\": (\"solid\", \"o\"),\n",
    "    \"ml\": (\"dashed\", \"s\"),\n",
    "    \"nwp\": (\"dotted\", \"^\"),\n",
    "}\n",
    "\n",
    "# Set global font sizes\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": FONT_SIZES[\"axes\"],\n",
    "    \"axes.titlesize\": FONT_SIZES[\"axes\"],\n",
    "    \"axes.labelsize\": FONT_SIZES[\"axes\"],\n",
    "    \"xtick.labelsize\": FONT_SIZES[\"ticks\"],\n",
    "    \"ytick.labelsize\": FONT_SIZES[\"ticks\"],\n",
    "    \"legend.fontsize\": FONT_SIZES[\"legend\"],\n",
    "    \"figure.titlesize\": FONT_SIZES[\"suptitle\"],\n",
    "})\n",
    "\n",
    "# Colorblind-friendly colormap for 2D plots\n",
    "COLORMAP = \"viridis\"\n",
    "\n",
    "# First, collect all the base variables and units we need to extend\n",
    "base_level_vars = {}\n",
    "for base_var, unit in VARIABLE_UNITS.items():\n",
    "    if \"_level\" in base_var:\n",
    "        base_level_vars[base_var] = unit\n",
    "\n",
    "# Then create the level-specific entries\n",
    "for level in REQUIRED_LEVELS:\n",
    "    for base_var, unit in base_level_vars.items():\n",
    "        VARIABLE_UNITS[f\"{base_var[:-len('_level')]}_{level}hPa\"] = unit\n",
    "print(f\"All units: {VARIABLE_UNITS}\")\n",
    "\n",
    "\n",
    "def save_plot(fig, name, time=None, remove_title=True, dpi=300):\n",
    "    \"\"\"Helper function to save plots consistently\n",
    "\n",
    "    Args:\n",
    "        fig: matplotlib figure object\n",
    "        name (str): base name for the plot file\n",
    "        time (datetime, optional): timestamp to append to filename\n",
    "        remove_title (bool): remove suptitle/title hierarchically if True\n",
    "        dpi (int): resolution for the saved figure, defaults to 300\n",
    "    \"\"\"\n",
    "    if time is not None:\n",
    "        name = f\"{name}_{time.dt.strftime('%Y%m%d_%H').values}\"\n",
    "\n",
    "    # Sanitize filename by replacing problematic characters\n",
    "    safe_name = name.replace(\"/\", \"_per_\")\n",
    "\n",
    "    # Normalize the path and ensure plots directory exists\n",
    "    plot_dir = Path(\"plots\")\n",
    "    plot_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Remove titles if requested\n",
    "    if remove_title:\n",
    "        if hasattr(fig, \"texts\") and fig.texts:  # Check for suptitle\n",
    "            fig.suptitle(\"\")\n",
    "        ax = fig.gca()\n",
    "        if ax.get_title():\n",
    "            ax.set_title(\"\")\n",
    "\n",
    "    pdf_path = plot_dir / f\"{safe_name}.pdf\"\n",
    "    fig.savefig(pdf_path, bbox_inches=\"tight\", dpi=dpi)\n",
    "\n",
    "\n",
    "def export_table(df, name, caption=\"\"):\n",
    "    \"\"\"Helper function to export tables consistently\"\"\"\n",
    "    # Export to LaTeX with caption\n",
    "    latex_str = df.to_latex(\n",
    "        float_format=\"%.4f\", caption=caption, label=f\"tab:{name}\"\n",
    "    )\n",
    "    with open(f\"tables/{name}.tex\", \"w\") as f:\n",
    "        f.write(latex_str)\n",
    "\n",
    "    # Export to CSV\n",
    "    df.to_csv(f\"tables/{name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ebba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ml = xr.open_zarr(PATH_ML)\n",
    "ds_ml = ds_ml.sel(state_feature=list(VARIABLES_ML.keys()))\n",
    "ds_ml = ds_ml.sel(y=slice(*Y), x=slice(*X))\n",
    "ds_ml = ds_ml.sel(start_time=slice(*START_TIMES))\n",
    "for feature in ds_ml.state_feature.values:\n",
    "    ds_ml[VARIABLES_ML[feature]] = ds_ml[\"state\"].sel(state_feature=feature)\n",
    "forecast_times = (\n",
    "    ds_ml.start_time.values[:, None] + ds_ml.elapsed_forecast_duration.values\n",
    ")\n",
    "ds_ml = ds_ml.assign_coords(\n",
    "    forecast_time=(\n",
    "        (\"start_time\", \"elapsed_forecast_duration\"),\n",
    "        forecast_times,\n",
    "    )\n",
    ")\n",
    "ds_ml = ds_ml.drop_vars([\"state\", \"state_feature\", \"time\"])\n",
    "ds_ml = ds_ml.transpose(\"start_time\", \"elapsed_forecast_duration\", \"x\", \"y\")\n",
    "ds_ml = ds_ml[\n",
    "    [\n",
    "        \"start_time\",\n",
    "        \"elapsed_forecast_duration\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        *VARIABLES_ML.values(),\n",
    "    ]\n",
    "]\n",
    "ds_ml = ds_ml.isel(elapsed_forecast_duration=ELAPSED_FORECAST_DURATION)\n",
    "\n",
    "ds_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf42e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gt = xr.open_zarr(PATH_GROUND_TRUTH)\n",
    "ds_gt = ds_gt.set_index(grid_index=[\"y\", \"x\"]).unstack(\"grid_index\")\n",
    "ds_gt = ds_gt.sel(y=slice(*Y), x=slice(*X))\n",
    "ds_gt = ds_gt.sel(state_feature=list(VARIABLES_ML.keys()))\n",
    "ds_gt = ds_gt.sel(split_name=\"test\").drop_dims([\n",
    "    \"forcing_feature\",\n",
    "    \"static_feature\",\n",
    "    \"split_part\",\n",
    "])\n",
    "for feature in ds_gt.state_feature.values:\n",
    "    ds_gt[VARIABLES_ML[feature]] = ds_gt[\"state\"].sel(state_feature=feature)\n",
    "ds_gt = ds_gt.drop_vars([\n",
    "    \"state\",\n",
    "    \"state_feature\",\n",
    "    \"state_feature_units\",\n",
    "    \"state_feature_long_name\",\n",
    "    \"state_feature_source_dataset\",\n",
    "    \"state__train__diff_mean\",\n",
    "    \"state__train__diff_std\",\n",
    "    \"state__train__mean\",\n",
    "    \"state__train__std\",\n",
    "])\n",
    "ds_gt = ds_gt.transpose(\"time\", \"x\", \"y\")\n",
    "ds_gt = ds_gt[\n",
    "    [\n",
    "        \"time\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        *VARIABLES_GROUND_TRUTH.values(),\n",
    "    ]\n",
    "]\n",
    "ds_gt = ds_gt.sel(time=np.unique(ds_ml.forecast_time.values.flatten()))\n",
    "ds_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_nwp = xr.open_zarr(PATH_NWP)\n",
    "ds_nwp = ds_nwp.sel(y=slice(*Y), x=slice(*X), analysis_time=slice(*START_TIMES))\n",
    "ds_nwp = ds_nwp[VARIABLES_NWP.keys()].rename(VARIABLES_NWP)\n",
    "ds_nwp = ds_nwp.rename_dims({\n",
    "    \"analysis_time\": \"start_time\",\n",
    "})\n",
    "ds_nwp = ds_nwp.rename_vars({\n",
    "    \"analysis_time\": \"start_time\",\n",
    "})\n",
    "forecast_times = (\n",
    "    ds_nwp.start_time.values[:, None] + ds_nwp.elapsed_forecast_duration.values\n",
    ")\n",
    "ds_nwp = ds_nwp.assign_coords(\n",
    "    forecast_time=(\n",
    "        (\"start_time\", \"elapsed_forecast_duration\"),\n",
    "        forecast_times,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Do not need these, remove so no issues later\n",
    "ds_nwp = ds_nwp.drop([\"time\"])\n",
    "\n",
    "# The NWP data starts at elapsed forecast duration 0 = start_time\n",
    "ds_nwp = ds_nwp.isel(\n",
    "    elapsed_forecast_duration=ELAPSED_FORECAST_DURATION_SHORT\n",
    ")\n",
    "\n",
    "ds_nwp = ds_nwp.transpose(\"start_time\", \"elapsed_forecast_duration\", \"x\", \"y\")\n",
    "ds_nwp = ds_nwp[\n",
    "    [\n",
    "        \"start_time\",\n",
    "        \"elapsed_forecast_duration\",\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        *VARIABLES_NWP.values(),\n",
    "    ]\n",
    "]\n",
    "\n",
    "ds_nwp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46114d2",
   "metadata": {},
   "source": [
    "Check for missing data in any of the variables. If you have missing data, you need to handle it before running the verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b77c2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if CHECK_MISSING:\n",
    "    missing_counts = dask.compute(\n",
    "        {var: ds_gt[var].isnull().sum().values for var in ds_gt.data_vars},\n",
    "        {var: ds_nwp[var].isnull().sum().values for var in ds_nwp.data_vars},\n",
    "        {var: ds_ml[var].isnull().sum().values for var in ds_ml.data_vars},\n",
    "    )\n",
    "    # Unpack results\n",
    "    gt_missing, nwp_missing, ml_missing = missing_counts\n",
    "\n",
    "    # Print results\n",
    "    print(\"Ground Truth\")\n",
    "    for var, count in gt_missing.items():\n",
    "        print(f\"{var}: {count} missing values\")\n",
    "\n",
    "    print(\"\\nNWP Model\")\n",
    "    for var, count in nwp_missing.items():\n",
    "        print(f\"{var}: {count} missing values\")\n",
    "\n",
    "    print(\"\\nML Model\")\n",
    "    for var, count in ml_missing.items():\n",
    "        print(f\"{var}: {count} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b36ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ds_gt.sizes[\"x\"] == ds_ml.sizes[\"x\"]\n",
    "assert ds_gt.sizes[\"x\"] == ds_nwp.sizes[\"x\"]\n",
    "assert ds_gt.sizes[\"y\"] == ds_ml.sizes[\"y\"]\n",
    "assert ds_gt.sizes[\"y\"] == ds_nwp.sizes[\"y\"]\n",
    "assert ds_gt.sizes[\"time\"] == len(\n",
    "    np.unique(ds_ml.forecast_time.values.flatten())\n",
    ")\n",
    "# Since nwp has less lead times below might not hold\n",
    "#assert ds_gt.sizes[\"time\"] == len(\n",
    "#    np.unique(ds_nwp.forecast_time.values.flatten())\n",
    "#)\n",
    "\n",
    "# Generate random indices for each dimension\n",
    "rng = np.random.RandomState(RANDOM_SEED)\n",
    "sampled_start_time_indices = np.sort(\n",
    "    rng.choice(\n",
    "        len(ds_ml.start_time),\n",
    "        size=int(len(ds_ml.start_time) * SUBSAMPLE_FRACTION),\n",
    "        replace=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "# with (\n",
    "#     LocalCluster(\n",
    "#         n_workers=4,\n",
    "#         threads_per_worker=16,\n",
    "#         memory_limit=\"96GB\",\n",
    "#         local_directory=\"/iopsstor/scratch/cscs/sadamov\",  # Use fast local storage for spilling\n",
    "#         dashboard_address=None,\n",
    "#     ) as cluster\n",
    "# ):\n",
    "#     with Client(cluster) as client:\n",
    "ds_ml_sampled = ds_ml.isel(start_time=sampled_start_time_indices)\n",
    "ds_nwp_sampled = ds_nwp.isel(start_time=sampled_start_time_indices)\n",
    "ds_gt_sampled = ds_gt.sel(time=ds_ml_sampled.forecast_time)\n",
    "if PRECOMPUTE_DATA:\n",
    "    with ProgressBar():\n",
    "        print(\"Computing ML data\")\n",
    "        ds_ml_sampled = ds_ml_sampled.compute()\n",
    "        print(\"Computing NWP data\")\n",
    "        ds_nwp_sampled = ds_nwp_sampled.compute()\n",
    "        print(\"Computing GT data\")\n",
    "        ds_gt_sampled = ds_gt_sampled.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f9ff87",
   "metadata": {},
   "source": [
    "### 3. Histograms\n",
    "By examining these distributions, we can assess whether the ML model and NWP model accurately capture the variability and frequency of different atmospheric states.\n",
    "\n",
    "**Distribution Shape:** The histograms show whether the models replicate the skewness, kurtosis, and overall shape of the ground truth data distributions.\n",
    "\n",
    "**Extreme Values:** Identifying how the models handle extreme conditions, such as unusually high or low temperatures, is crucial for weather prediction and risk assessment.\n",
    "\n",
    "**Normalization Needs:** Differences in scale between variables suggest that normalization may be necessary for accurate comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable_name in VARIABLES_GROUND_TRUTH.values():\n",
    "    fig, ax = plt.subplots(figsize=(16, 7), dpi=DPI)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    data_gt = ds_gt_sampled.isel(elapsed_forecast_duration=ELAPSED_FORECAST_DURATION_SHORT)[variable_name].values.flatten()\n",
    "    data_ml = ds_ml_sampled.isel(elapsed_forecast_duration=ELAPSED_FORECAST_DURATION_SHORT)[variable_name].values.flatten()\n",
    "\n",
    "    # Create histograms for ground truth\n",
    "    ax.hist(\n",
    "        data_gt,\n",
    "        bins=200,\n",
    "        density=True,\n",
    "        color=COLORS[\"gt\"],\n",
    "        label=\"Ground Truth\",\n",
    "        histtype=\"stepfilled\",  # Use filled steps without edges\n",
    "        linewidth=0,\n",
    "    )\n",
    "\n",
    "    # Plot NWP if available\n",
    "    if variable_name in ds_nwp:\n",
    "        data_nwp = ds_nwp_sampled[variable_name].values.flatten()\n",
    "        ax.hist(\n",
    "            data_nwp,\n",
    "            bins=200,\n",
    "            alpha=0.8,\n",
    "            density=True,\n",
    "            color=COLORS[\"nwp\"],\n",
    "            label=\"NWP Model Prediction\",\n",
    "            histtype=\"stepfilled\",\n",
    "            linewidth=0,\n",
    "        )\n",
    "\n",
    "    # Create histogram for ML\n",
    "    ax.hist(\n",
    "        data_ml,\n",
    "        bins=200,\n",
    "        alpha=0.8,\n",
    "        density=True,\n",
    "        color=COLORS[\"ml\"],\n",
    "        label=\"ML Model Prediction\",\n",
    "        histtype=\"stepfilled\",\n",
    "        linewidth=0,\n",
    "    )\n",
    "\n",
    "    # Add labels and title with adjusted positioning\n",
    "    units = VARIABLE_UNITS[variable_name]\n",
    "    ax.set_title(\n",
    "        f\"Distribution of {variable_name}\", pad=20\n",
    "    )  # Add padding below title\n",
    "    ax.set_xlabel(f\"{units}\")  # Add units to x-axis label\n",
    "\n",
    "    # Place legend in top left with some padding from the edge\n",
    "    ax.legend(loc=\"upper left\", bbox_to_anchor=(0.02, 0.98))\n",
    "\n",
    "    # Adjust axis limits to leave space for both legend and table\n",
    "    current_ylim = ax.get_ylim()\n",
    "    ax.set_ylim(0, current_ylim[1] * 1.3)  # Increased padding for both elements\n",
    "\n",
    "    # Calculate statistics\n",
    "    stats_data = {\n",
    "        \"GT\": [f\"{skew(data_gt):.2f}\", f\"{kurtosis(data_gt):.2f}\"],\n",
    "        \"ML\": [f\"{skew(data_ml):.2f}\", f\"{kurtosis(data_ml):.2f}\"],\n",
    "    }\n",
    "    if variable_name in ds_nwp:\n",
    "        stats_data[\"NWP\"] = [\n",
    "            f\"{skew(data_nwp):.2f}\",\n",
    "            f\"{kurtosis(data_nwp):.2f}\",\n",
    "        ]\n",
    "\n",
    "    # Create and position table in top right with adjusted dimensions\n",
    "    col_labels = [\"Skewness\", \"Kurtosis\"]\n",
    "    row_labels = list(stats_data.keys())\n",
    "    cell_text = [[stats_data[row][i] for i in range(2)] for row in row_labels]\n",
    "\n",
    "    table = ax.table(\n",
    "        cellText=cell_text,\n",
    "        rowLabels=row_labels,\n",
    "        colLabels=col_labels,\n",
    "        cellLoc=\"center\",\n",
    "        loc=\"upper right\",\n",
    "        bbox=[\n",
    "            0.68,\n",
    "            0.6,\n",
    "            0.3,\n",
    "            0.35,\n",
    "        ],  # Adjust position (x, y, width, height)\n",
    "    )\n",
    "\n",
    "    # Enhanced table styling\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(22)\n",
    "\n",
    "    # Adjust column widths and row heights\n",
    "    for (row, col), cell in table._cells.items():\n",
    "        cell.set_text_props(wrap=True)\n",
    "        cell.set_facecolor(\"white\")\n",
    "        cell.set_alpha(0.9)\n",
    "        cell.set_edgecolor(\"#D3D3D3\")  # Light gray color\n",
    "\n",
    "        # Adjust column widths and alignment\n",
    "        if col == -1:  # Row header column\n",
    "            cell.set_text_props(horizontalalignment=\"right\")  # Right align text\n",
    "        else:  # Data columns\n",
    "            cell.set_text_props(\n",
    "                horizontalalignment=\"center\"\n",
    "            )  # Keep center alignment for data\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "    save_plot(fig, f\"histogram_{variable_name}\")\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
